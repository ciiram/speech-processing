{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "### Introduction\n",
    "In classification, we assign an input vector $\\mathbf{x}\\in \\mathbf{R}^d$ to one of $K$ classes. That is we aim to learn a function \n",
    "\n",
    "$f:\\mathbf{R}^d\\rightarrow [K]$\n",
    "\n",
    "#### Simple Classifiers\n",
    "\n",
    "Let us get some intuition for the classification problem by generating some toy data from two classes. These are two widely separated Gaussian blobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_class = 2\n",
    "num_per_class = 10\n",
    "var = 1\n",
    "means = [[1,1],[-1,-1]]\n",
    "cov = [[var, 0], [0, var]] \n",
    "X = np.array([])\n",
    "y = []\n",
    "class_color = ['b','r']\n",
    "\n",
    "for class_index in range(num_class):\n",
    "    class_data = np.random.multivariate_normal(means[class_index],\n",
    "                                               cov, \n",
    "                                               num_per_class)\n",
    "    X = np.vstack([X, class_data]) if X.size else class_data\n",
    "    y = np.concatenate((y, np.ones(num_per_class) * class_index))\n",
    "    plt.plot(class_data[:, 0], \n",
    "             class_data[:,1], \n",
    "             class_color[class_index] + 'o')\n",
    "    plt.axis('equal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, if we can draw a straight line separating the two classes, a ideal classifier would assign points on one side of the line to one class and those on the other side to the other class. We would use training data to learn the appropriate line. Problems where we can separate classes using lines (hyperplanes) are said to be *linearly separable* and represent \"simple\" cases. Often we are unable to draw lines to separate classes. However, state of the art machine learning methods are still able to work in this cases.\n",
    "\n",
    "### Nearest Neighbour Classifier\n",
    "After training our classifier, we aim to be able to correctly classify new and previously unseen examples. Returning to the previous example, let us generate a random point from one of the two classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_class = np.random.binomial(1,.5) # either class 1 or 0\n",
    "test_data = np.random.multivariate_normal(means[data_class], cov, 1)\n",
    "\n",
    "plt.plot(X[np.array(y)==0,0], X[np.array(y)==0,1], class_color[0] + 'o')\n",
    "plt.plot(X[np.array(y)==1,0], X[np.array(y)==1,1], class_color[1] + 'o')\n",
    "plt.plot(test_data[0,0], test_data[0,1], 'x', markersize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can get a very good classifier by assigning the class of the nearest training point to the test point. This is the nearest neighbour classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nearest neighbour classifier\n",
    "dist = np.sqrt(np.sum((X - test_data)**2, 1))\n",
    "print('True Test Class: %d Predicted Test Class %d'%(data_class, y[np.argmin(dist)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neighbour classifier\n",
    "\n",
    "Instead of using the class of the nearest training point, we can examine the classes of the K nearest neighbours and assign a class to the test point by majority voting.\n",
    "\n",
    "To determine the best value of K, we examine the classification accuracy on a validation set. In the following, we use the [scikit learn](http://scikit-learn.org/stable/index.html) implementation of a K nearest neighbour classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set - learn classifier\n",
    "Validation set - choose optimal parameter setting\n",
    "Test set - assess performance of the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # to obtain the train, validation and test split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.4)# 60% for training\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=.5) # 20% validation and 20 % test\n",
    "\n",
    "neighbours = [1, 2, 5, 10] # number of neighbours to test\n",
    "accuracy = np.zeros(len(neighbours))\n",
    "\n",
    "for index in range(len(neighbours)):\n",
    "    clf = KNeighborsClassifier(n_neighbors=neighbours[index])\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy[index] = accuracy_score(y_val, clf.predict(X_val))\n",
    "\n",
    "plt.plot(neighbours, accuracy)\n",
    "\n",
    "#chose K\n",
    "K = neighbours[np.argmax(accuracy)]\n",
    "#Get the performance on test data\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=K)\n",
    "clf.fit(X_train, y_train)\n",
    "print('Best value of K is:%d'%K)\n",
    "print('Accuracy on test data %.2f'%accuracy_score(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying digits\n",
    "\n",
    "Let us attempt to build a KNN classifier for digits. We will use one of the datasets already available of scikit learn. The digits are 8-by-8 images stored in an array of length 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data set and plot some samples\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "X_digits = digits.images\n",
    "y_digits = digits.target\n",
    "plt.gray();\n",
    "plt.matshow(X_digits[0]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the procedure of dividing the data into training, validation \n",
    "# and test set and train a K nearest neighbour classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat this using the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
