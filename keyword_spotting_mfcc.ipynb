{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Word Spotting\n",
    "\n",
    "This notebook describes the training of a KWS system using the data from the [TensorFlow Speech Recognition Challenge](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/data).\n",
    "\n",
    "We will use a neural network classifier.\n",
    "\n",
    "First we get the data and extract it.\n",
    "\n",
    "In the following code you may need to modify the directory structure depending on your system and create directories to hold the data and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "url = 'http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz'\n",
    "data_files = os.listdir('../data/')\n",
    "\n",
    "if 'speech_commands_v0.01.tar.gz' not in data_files:\n",
    "    print('Downloading...')\n",
    "    urllib.request.urlretrieve(url, '../data/speech_commands_v0.01.tar.gz')\n",
    "    print('Done!')\n",
    "else:\n",
    "    print('Data Available')\n",
    "    \n",
    "# unzip\n",
    " \n",
    "if 'yes' not in data_files:\n",
    "    try:    \n",
    "        tf = tarfile.open('../data/speech_commands_v0.01.tar.gz')\n",
    "        tf.extractall('../data/')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now obtain a random file with the word \"yes\" and plot its waveform and spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import librosa\n",
    "import librosa.display\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# set up speech processing parameters\n",
    "NFFT = 512  # 32 ms at 16kHz\n",
    "HOP_LENGTH = 256 \n",
    "NUM_MFCC = 13\n",
    "\n",
    "\n",
    "yes_files = os.listdir('../data/yes')\n",
    "signal, sampling_rate = librosa.load('../data/yes/' + random.choice(yes_files), sr=None) # use native sampling rate\n",
    "    \n",
    "spectrum = np.abs(librosa.stft(signal, n_fft=NFFT, hop_length=HOP_LENGTH))\n",
    "mfccs = librosa.feature.mfcc(y=signal, sr= sampling_rate, n_mfcc=NUM_MFCC, n_fft=NFFT, hop_length=HOP_LENGTH)\n",
    "mfccs = mfccs[1:, :]  # discard the zeroeth coefficient which doesn't have much speaker specific information \n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "librosa.display.specshow(librosa.amplitude_to_db(spectrum, ref=np.max),\n",
    "                         sr=sampling_rate,\n",
    "                         hop_length=HOP_LENGTH,\n",
    "                         y_axis='linear', \n",
    "                         x_axis='time')\n",
    "plt.title('Spectrogram of \"Yes\"')\n",
    "plt.xticks([])\n",
    "plt.xlabel('')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(np.arange(len(signal)) / sampling_rate, signal)\n",
    "plt.xlim([0, 1]);\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "librosa.display.specshow(mfccs,\n",
    "                         sr=sampling_rate,\n",
    "                         hop_length=HOP_LENGTH,\n",
    "                         y_axis='linear', \n",
    "                         x_axis='time')\n",
    "plt.title('MFCCs of \"Yes\"')\n",
    "plt.xticks([])\n",
    "plt.xlabel('')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(np.arange(len(signal)) / sampling_rate, signal)\n",
    "plt.xlim([0, 1]);\n",
    "plt.savefig('audio_mfcc.jpg', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "693 / 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "\n",
    "We determine the number of files for each of the words. The core words in the dataset are \"Yes\", \"No\", \"Up\", \"Down\", \"Left\", \"Right\", \"On\", \"Off\", \"Stop\", \"Go\", \"Zero\", \"One\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\", and \"Nine\". There are ten auxiliary words: \"Bed\", \"Bird\", \"Cat\", \"Dog\", \"Happy\", \"House\", \"Marvin\", \"Sheila\", \"Tree\", and \"Wow\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_words = ['yes', 'no', 'up', 'down', \n",
    "              'left', 'right', 'on', 'off', \n",
    "              'stop', 'go', 'zero', 'one', \n",
    "              'two', 'three', 'four', 'five',\n",
    "             'six', 'seven', 'eight', 'nine']\n",
    "\n",
    "auxiliary_words = ['bed', 'bird', 'cat', 'dog',\n",
    "                  'happy', 'house', 'marvin', 'sheila',\n",
    "                  'tree', 'wow']\n",
    "\n",
    "filenames = []\n",
    "for word in core_words + auxiliary_words:\n",
    "    word_files = os.listdir('../data/' + word)\n",
    "    for file in word_files:\n",
    "        filenames.append(word + '/' + file)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(auxiliary_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get features from data.\n",
    "\n",
    "First we use MFCCs which we flatten into a vector. All files are assumed to be one second long and are either clipped or zero-padded to meet this criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_files = os.listdir('../data/features/')\n",
    "\n",
    "if 'mfccs_' + str(NUM_MFCC - 1) + '.npy' not in feature_files:\n",
    "    features = np.array([])\n",
    "    start_time = time.time()\n",
    "    for index, file in enumerate(filenames):\n",
    "        if not index % 100:\n",
    "            print('Processing file {} of {}'.format(index, len(filenames)))\n",
    "        signal, sampling_rate = librosa.load('../data/' + file, sr=None)\n",
    "    \n",
    "        # Ensure that the signal is one second long\n",
    "        if len(signal) > sampling_rate:\n",
    "            signal = signal[:sampling_rate]\n",
    "        elif len(signal) < sampling_rate:\n",
    "            signal = np.concatenate((signal, np.zeros(sampling_rate - len(signal))))\n",
    "        \n",
    "        assert(len(signal)==sampling_rate)\n",
    "        mfccs = librosa.feature.mfcc(y=signal, \n",
    "                                     sr= sampling_rate, \n",
    "                                     n_mfcc=NUM_MFCC, \n",
    "                                     n_fft=NFFT, \n",
    "                                     hop_length=HOP_LENGTH)\n",
    "    \n",
    "        if features.size:\n",
    "            features = np.vstack((features, mfccs[1:, :].reshape(-1,)))  # eliminate zeroeth coefficient\n",
    "        else:\n",
    "            features = mfccs[1:, :].reshape(-1,)\n",
    "\n",
    "    stop_time = time.time()\n",
    "    print('Done in {} seconds. {} per file!'.format(stop_time - start_time, (stop_time - start_time) / len(filenames) ))\n",
    "    \n",
    "    normed_features = (features.T / (np.max(np.abs(features), axis=1) + 1e-8)).T\n",
    "    \n",
    "    # save the features\n",
    "    np.save('../data/features/mfccs_' + str(NUM_MFCC -  1), features)\n",
    "\n",
    "else:\n",
    "    features = np.load('../data/features/mfccs_' + str(NUM_MFCC - 1) + '.npy')\n",
    "    normed_features = (features.T / (np.max(np.abs(features), axis=1) + 1e-8)).T\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(311)\n",
    "librosa.display.specshow(features[0, :].reshape((NUM_MFCC - 1 , -1)),\n",
    "                         sr=sampling_rate,\n",
    "                         hop_length=HOP_LENGTH,\n",
    "                         y_axis='linear', \n",
    "                         x_axis='time')\n",
    "plt.title('MFCCs')\n",
    "plt.xticks([])\n",
    "plt.xlabel('')\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(features[0, :])\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(normed_features[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[0, :].reshape((NUM_MFCC - 1 , -1)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12 *63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = len(core_words + auxiliary_words)\n",
    "\n",
    "\n",
    "# Get training, validation and test\n",
    "num_files = len(filenames)\n",
    "\n",
    "training_indices = random.sample(range(num_files), int(0.8 * num_files))\n",
    "training_indices.sort()\n",
    "\n",
    "test_and_val = list(set(range(num_files)) - set(training_indices))\n",
    "validation_indices = random.sample(test_and_val, int(0.5 * len(test_and_val)))\n",
    "test_indices = list(set(test_and_val) - set(validation_indices))\n",
    "validation_indices.sort()\n",
    "test_indices.sort()\n",
    "\n",
    "assert(len(training_indices) + len(validation_indices) + len(test_indices) == num_files)\n",
    "\n",
    "\n",
    "X_train = normed_features[training_indices, :]\n",
    "X_val = normed_features[validation_indices, :]\n",
    "X_test = normed_features[test_indices, :]\n",
    "\n",
    "# get the targets\n",
    "all_words = core_words + auxiliary_words\n",
    "labels = []\n",
    "for file in filenames:\n",
    "    labels.append(all_words.index(file.split('/')[0]))\n",
    "    \n",
    "y_train = np.array(labels)[training_indices]\n",
    "y_val = np.array(labels)[validation_indices]\n",
    "y_test = np.array(labels)[test_indices]\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "# Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "# Y_val = np_utils.to_categorical(y_val, nb_classes)\n",
    "# Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HL=(500,)\n",
    "\n",
    "\n",
    "#create the classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=HL, \n",
    "                    activation='relu',\n",
    "                    max_iter=50, \n",
    "                    alpha=1e-4, \n",
    "                    solver='sgd', \n",
    "                    verbose=10, \n",
    "                    tol=1e-4, \n",
    "                    random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print(\"Validation set score: %f\" % mlp.score(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.predict_log_proba(X_test[0].reshape(1, -1))[0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(10), mlp.predict_proba(X_test[2000].reshape(1, -1))[0,:10])\n",
    "plt.xticks(np.arange(10), core_words[:10]);\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Probability')\n",
    "plt.savefig('word_prob.jpg', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
