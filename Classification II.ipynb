{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification \n",
    "### Introduction\n",
    "Last week we introduced classification where we assign an input vector $\\mathbf{x}\\in \\mathbf{R}^d$ to one of $K$ classes. That is we aim to learn a function \n",
    "\n",
    "$f:\\mathbf{R}^d\\rightarrow [K]$\n",
    "\n",
    "#### Simple Classifiers\n",
    "\n",
    "Consider a two dimensional example where data are drawn from two classes. If we can draw a straight line separating the two classes, a ideal classifier would assign points on one side of the line to one class and those on the other side to the other class. We would use training data to learn the appropriate line. Problems where we can separate classes using lines (hyperplanes) are said to be *linearly separable*\n",
    "\n",
    "Consider the sample data set below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_class = 2\n",
    "num_per_class = 10\n",
    "var = 0.1\n",
    "means = [[1,1],[-1,-1]]\n",
    "cov = [[var, 0], [0, var]] \n",
    "X = np.array([])\n",
    "y = []\n",
    "class_color = ['b','r']\n",
    "\n",
    "for class_index in range(num_class):\n",
    "    class_data = np.random.multivariate_normal(means[class_index],\n",
    "                                               cov, \n",
    "                                               num_per_class)\n",
    "    X = np.vstack([X, class_data]) if X.size else class_data\n",
    "    y = np.concatenate((y, np.ones(num_per_class) * class_index))\n",
    "    plt.plot(class_data[:, 0], \n",
    "             class_data[:,1], \n",
    "             class_color[class_index] + 'o')\n",
    "    plt.xlim([-2, 2])\n",
    "    plt.ylim([-2, 2])\n",
    "    plt.xlabel(r'$x_1$', fontsize=14)\n",
    "    plt.ylabel(r'$x_2$', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data $\\mathbf{x}_n = [x_{n1}, x_{n2}]^T$ for $n=1,\\ldots,N$ are drawn from two Gaussian blobs. In this case we can draw a line separating the two classes. \n",
    "\n",
    "Let \n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "y = w_1x_1 + w_2x_2 + w_0\n",
    "\\end{equation}$\n",
    "\n",
    "If we set $w_1=1, w_2=1$ and $w_0=0$ we can separate the classes. We use the following rule\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\mathrm{Class}=\\left\\{ \\begin{array}{ll}\n",
    "\\mathrm{Blue} & \\textrm{ $y> 0$}\\\\\n",
    "\\mathrm{Red} & \\textrm{$y<0$}\n",
    "\\end{array} \\right.\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "The classifier corresponds to \n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "y(\\mathbf{x}) = f(w_1x_1 + w_2x_2 + w_0)\n",
    "\\end{equation}\n",
    "$ \n",
    "\n",
    "where $f(.)$ is the nonlinear activation function given by the unit step\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "f(a)=\\left\\{ \\begin{array}{ll}\n",
    "1 & \\textrm{ $a> 0$}\\\\\n",
    "0 & \\textrm{$a<0$}\n",
    "\\end{array} \\right.\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Where 1 corresponds to the blue class and 0 to the red class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# straight line\n",
    "x1 = np.linspace(-2, 2, 100)\n",
    "x2 = -x1\n",
    "for class_index in range(num_class):\n",
    "    plt.plot(X[y==class_index, 0], \n",
    "             X[y==class_index, 1], \n",
    "             class_color[class_index] + 'o')\n",
    "    plt.plot(x1, x2)\n",
    "    plt.xlim([-2, 2])\n",
    "    plt.ylim([-2, 2])\n",
    "    plt.ylabel(r'$x_2$', fontsize=14)\n",
    "    plt.xlabel(r'$x_1$', fontsize=14)\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "f(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x} \n",
    "$\n",
    "\n",
    "Let $\\mathbf{w} = [1, 1]^T$, $\\mathbf{x} = [0.5, -0.5]^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The perceptron algorithm\n",
    "\n",
    "In general linearly separable cases, it will not be easy to find the equation\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "y(\\mathbf{x}) = f(\\mathbf{w}^T\\mathbf{x} + w_0)\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "that separates the classes. We will consider the perceptron algorithm of Rosenblatt which can find the solution to linearly separable classification problems.\n",
    "\n",
    "In the perceptron we have\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "y(\\mathbf{x}) = f(\\mathbf{w}^T\\phi(\\mathbf{x}))\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "where $\\phi(x)$ is a nonlinear transformation of the input vector $\\mathbf{x}$ and $f(.)$ is given by the step function\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "f(a)=\\left\\{ \\begin{array}{ll}\n",
    "+1 & \\textrm{ $a \\geq 0$}\\\\\n",
    "-1 & \\textrm{$a<0$}\n",
    "\\end{array} \\right.\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "We use the target values $t=+1$ for class one and $t=-1$ for class 2. So the target $t\\in \\{+1, -1\\}$. \n",
    "\n",
    "We seek a weight vector $\\mathbf{w}$ such that for patterns in class one, $\\mathbf{w}^T\\phi(\\mathbf{x})>0$ and for patterns in class two, $\\mathbf{w}^T\\phi(\\mathbf{x})<0$ Thus for all patterns correct $\\mathbf{w}$ will ensure \n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\mathbf{w}^T\\phi(\\mathbf{x}_n)t_n>0\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "We now introduce the error function which will guide the *learning* of $\\mathbf{w}$. We assign zero error to correctly classified examples and try to minimize $-\\mathbf{w}^T\\phi(\\mathbf{x}_n)t_n$ for misclassified examples. The error measure also known as the perceptron criterion is given by\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "E_P(\\mathbf{w}) = -\\sum_{n\\in\\mathcal{M}}\\mathbf{w}^T\\phi(\\mathbf{x}_n)t_n\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Where $\\mathcal{M}$ is the set of misclassified examples.\n",
    "\n",
    "#### Learning - stochastic gradient descent\n",
    "We have that\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "E_P(\\mathbf{w}) = \\sum_{n\\in\\mathcal{M}}E_P^n(\\mathbf{w})\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "where $E_P^n(\\mathbf{w})=-\\mathbf{w}^T\\phi(\\mathbf{x}_n)t_n$.\n",
    "\n",
    "We apply stochastic gradient descent starting from an initial weight vector and present the examples one at a time and adjust the weights according to\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\mathbf{w}^{(\\tau + 1)}=\\mathbf{w}^{(\\tau)} - \\eta\\nabla E_P^n(\\mathbf{w}) \n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "where\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\nabla E_P^n(\\mathbf{w})=\\left\\{ \\begin{array}{ll}\n",
    "-\\phi_nt_n &  n\\in\\mathcal{M}\\\\\n",
    "0 & n\\notin\\mathcal{M}\n",
    "\\end{array} \\right.\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "#### Let's code it up\n",
    "Let's code up the algorithm assuming $\\eta = 1$ and $\\phi(x) = x$. We also incorporate the bias term $w_0$ in the weight vector by introducing $\\phi_0(x) = 1$. Thus $\\mathbf{w} = [w_0, w_1, w_2]^T$ and $\\mathbf{x} = [1, x_1, x_2]$.\n",
    "\n",
    "We define a function that takes in the current training input, current weight vector and target value and returns the gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_gradient(input_vector, weight, target):\n",
    "    '''Compute the gradient of the perceptron error function\n",
    "    Args:\n",
    "        input_vector: the input vector x\n",
    "        weight: the weight vector w\n",
    "        target: the target value +1 or -1\n",
    "    Returns:\n",
    "        gradient: the gradient at the input vector\n",
    "    '''\n",
    "    if np.dot(weight, input_vector) * target < 0:\n",
    "        return -input_vector * target\n",
    "    else:\n",
    "        return np.zeros(len(input_vector))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add x_0 = 1 for all examples\n",
    "X_new = np.vstack((np.ones(X.shape[0]), X.T)).T\n",
    "# change the targets to +1, -1 for 1, 0\n",
    "y_new = 2 * y - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the algorithm starting at $\\mathbf{w} = [0, 1, -1]$. We see that initially some examples are misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# straight line\n",
    "x1 = np.linspace(-2, 2, 100)\n",
    "x2 = x1\n",
    "for class_index in range(num_class):\n",
    "    plt.plot(X[y==class_index, 0], \n",
    "             X[y==class_index, 1], \n",
    "             class_color[class_index] + 'o')\n",
    "    plt.plot(x1, x2)\n",
    "    plt.xlim([-2, 2])\n",
    "    plt.ylim([-2, 2])\n",
    "    plt.xlabel(r'$x_1$', fontsize=14)\n",
    "    plt.xlabel(r'$x_2$', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_vector = np.array([0, 1, -1])\n",
    "\n",
    "for index in range(X_new.shape[0]):\n",
    "    weight_vector = weight_vector - perceptron_gradient(X_new[index, :], \n",
    "                                                        weight_vector, \n",
    "                                                        y_new[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the line corresponding to this learned weight vector and verify that it separates the classes correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the line\n",
    "x2 = -x1 * (weight_vector[1] / weight_vector[2]) - (weight_vector[0] / weight_vector[2]) \n",
    "\n",
    "for class_index in range(num_class):\n",
    "    plt.plot(X[y==class_index, 0], \n",
    "             X[y==class_index, 1], \n",
    "             class_color[class_index] + 'o')\n",
    "    plt.plot(x1, x2)\n",
    "    plt.xlim([-2, 2])\n",
    "    plt.ylim([-2, 2])\n",
    "    plt.xlabel(r'$x_1$', fontsize=14)\n",
    "    plt.xlabel(r'$x_2$', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "This notebook is based on the description of the perceptron algorithm in section 4.1.7 in Bishop's book - [Pattern Recognition and Machine Learning](https://www.springer.com/gp/book/9780387310732)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
