{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Neural networks are a powerful machine learning framework used to learn complex input-output mappings from examples. Examples of successful applications of neural networks include:\n",
    "1. Classification of handwritten digits\n",
    "2. Speech recognition\n",
    "\n",
    "Neural networks can be viewed as a series of nonlinear transformations applied to the input variables where the nature of the transformation is learned from the training data. There are several neural network architectures but we will focus on a feedforward architecture where information flows in one direction from input to output and there is no feedback from the output back to the input.\n",
    "\n",
    "## The Multilayer Perceptron\n",
    "\n",
    "The multilayer perceptron (MLP) is a feed forward neural network. The figure below shows an MLP with a single hidden layer (from http://deeplearning.net/tutorial/mlp.html).\n",
    "![MLP](mlp.png)\n",
    "\n",
    "The input to a given layer is obtained from the output of the previous layer. The output of a given layer is obtained by applying an activation function to a weighted linear combination of the inputs. Mathematically let $x_1,\\ldots,x_D$ be the inputs to a given layer. The output of the $j$th hidden layer is given by\n",
    "\\begin{eqnarray*}\n",
    "z_j=h(\\sum_{i=1}^Dw_{ji}x_i+w_{j0})\n",
    "\\end{eqnarray*}\n",
    "where:\n",
    "1. $h(.)$ is a nonlinear activation function\n",
    "2. $w_{ji}$ is the weight from input node (neuron) $i$ to output node $j$\n",
    "3. $w_{j0}$ is known as the bias of neuron $j$\n",
    "\n",
    "Similarly, the output $y_k$ of the $k$th output neuron is obtained by applying an activation function to a weighted linear combination of the inputs from the hidden layer. This output is a function of the weights $\\mathbf{w}$ and the inputs $\\mathbf{x}$ and we write $y_k(\\mathbf{x},\\mathbf{w})$. We can collect all the outputs into a vector $\\mathbf{y}(\\mathbf{x},\\mathbf{w})$\n",
    "\n",
    "\n",
    "### Activation Functions\n",
    "There are a number of activation functions used depending on the nature of the data and target variables. These include:\n",
    "1. The sigmoid function \n",
    "\\begin{eqnarray*}\n",
    "\\sigma(a)=\\frac{1}{1+\\exp(-a)}\n",
    "\\end{eqnarray*}\n",
    "2. The Tanh function \n",
    "\\begin{eqnarray*}\n",
    "\\tanh(a)=\\frac{\\exp(a)-\\exp(-a)}{\\exp(a)+\\exp(-a)}\n",
    "\\end{eqnarray*}\n",
    "3. The rectified linear unit (ReLU)\n",
    "\\begin{eqnarray*}\n",
    "f(a)=max\\{o,a\\}\n",
    "\\end{eqnarray*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write three functions to plot the three activation functions. \n",
    "#The functions you write should take in a vector of points and return \n",
    "#the activation function evaluated at those points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Learning\n",
    "\n",
    "The aim of training the neural network is to learn an input-output mapping from examples. We aim to learn a set of weights and biases to obtain the appropriate mapping. Given $N$ training examples $\\mathbf{x}_n$ and the correspinding target output vectors $\\mathbf{t}_n$, we aim to learn weights and biases to minimize the error\n",
    "\\begin{eqnarray*}\n",
    "E(\\mathbf{w})=\\frac{1}{2}\\sum_{n=1}^N||\\mathbf{y}(\\mathbf{x}_n,\\mathbf{w})-\\mathbf{t}_n)||^2\n",
    "\\end{eqnarray*}\n",
    "\n",
    "This learning is often achieved by gradient descent where the weights at one time step $\\tau$ are modified in the direction of negative gradient according to \n",
    "\\begin{eqnarray*}\n",
    "\\mathbf{w}^{(\\tau+1)}=\\mathbf{w}^{(\\tau)}-\\eta\\nabla E(\\mathbf{w}^{(\\tau)})\n",
    "\\end{eqnarray*}\n",
    "where $\\eta>0$ is the learning rate. In practice, for the MLP the gradient of the error function is found by backpropagation.\n",
    "\n",
    "### Example\n",
    "We will consider the example of learning to classify hand written digits. We will use the MNIST training set which consists of 70,000 examples and the implementation of the MLP on scikit learn http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#from sklearn.datasets import fetch_mldata\n",
    "#mnist = fetch_mldata('MNIST original')\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore the data by plotting a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The flattened images of the digits are 784 dimensional arrays\n",
    "mnist.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(mnist.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The targets are stored in mnist.target\n",
    "mnist.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot a random digit and show the label\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "rand_index = np.random.randint(0,mnist.data.shape[0])\n",
    "plt.imshow(mnist.data[rand_index].reshape(28,28), cmap='Greys_r')\n",
    "plt.xticks([]);\n",
    "plt.yticks([]);\n",
    "print(int(mnist.target[rand_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now divide the data into training, validation and testing. Use the split 42k training, 14k validation and 14k for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write code to divide the data into training, validation and test sets\n",
    "#X_train,X_val,X_test=\n",
    "#y_test,y_val,y_test=\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data, \n",
    "                                                    mnist.target,\n",
    "                                                    test_size=.4)# 60% for training\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, \n",
    "                                                y_test,\n",
    "                                                test_size=.5) # 20% validation and 20 % test\n",
    "\n",
    "#Normalise the data by dividing the pixel values by 255\n",
    "\n",
    "X_train = np.array(X_train) / 255\n",
    "X_val = np.array(X_val) / 255\n",
    "X_test = np.array(X_test) / 255\n",
    "\n",
    "X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the data to select an appropriate number of hidden layer neurons\n",
    "#for an MLP with a single hidden layer the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the number of hidden neurons per layer as a tuple\n",
    "HL=(100,)\n",
    "\n",
    "\n",
    "#create the classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=HL, \n",
    "                    activation='tanh',\n",
    "                    max_iter=50, \n",
    "                    alpha=1e-4, \n",
    "                    solver='sgd', \n",
    "                    verbose=10, \n",
    "                    tol=1e-4, \n",
    "                    random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print(\"Validation set score: %f\" % mlp.score(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify the above code to search through MLPs with 50,100,150,200,500 and 1000 neurons\n",
    "#in the hidden layer to find the best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with the best network, learn a model and use it to find the accuracy \n",
    "#on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us view some of the digits in the test set and corresponding classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use mlp.predict to see how your classifier labels some test digits\n",
    "rand_index = np.random.randint(0, X_test.shape[0])\n",
    "plt.imshow(X_test[rand_index].reshape(28,28), cmap='Greys_r')\n",
    "plt.xticks([]);\n",
    "plt.yticks([]);\n",
    "mlp.predict(X_test[rand_index].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
